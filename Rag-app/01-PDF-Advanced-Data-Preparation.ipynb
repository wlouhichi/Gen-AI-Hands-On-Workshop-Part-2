{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbeea8b7-a4e4-47fd-87ac-32e5e58816d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbdemos\n",
    "dbutils.library.restartPython()\n",
    "import dbdemos\n",
    "dbdemos.create_cluster('llm-rag-chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbfdbd24-792f-452b-ada3-fec2af990588",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required external libraries "
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U transformers==4.41.1 pypdf==4.1.0 langchain-text-splitters==0.2.0 databricks-vectorsearch mlflow tiktoken==0.7.0 torch==2.3.0 llama-index==0.10.43 langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a34c0a-dfe7-47ed-889c-412faad3eb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42eb079d-6d04-4b76-b197-e36dff9e0e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Prepare the data by putting the news pdf document in volumes and create the eveluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32182071-ef15-4803-8d87-848600ab55d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch username dynamically\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"user\").get()\n",
    "\n",
    "# Create the volume if it does not exist\n",
    "try:\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{db}.pdfdata\")\n",
    "    print(\"Volume created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating volume: {e}\")\n",
    "    \n",
    "# Define source and target paths\n",
    "workspace_path = f\"file:/Workspace/Users/{username}/Gen-AI-Hands-On-Workshop-Part-2/Data/news.pdf\"\n",
    "volumes_path = f\"/Volumes/{catalog}/{db}/pdfdata/news.pdf\"\n",
    "\n",
    "# Verify source file existence\n",
    "print(f\"Checking existence of: {workspace_path}\")\n",
    "try:\n",
    "    dbutils.fs.head(workspace_path, 1)\n",
    "    print(\"File exists, proceeding with copy.\")\n",
    "    dbutils.fs.cp(workspace_path, volumes_path)\n",
    "    print(\"File copied successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d94c12a-f059-410e-bee6-77822b113144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "workspace_file_path = f\"file:/Workspace/Users/{username}/Gen-AI-Hands-On-Workshop-Part-2/Data/eval_dataset.csv\"\n",
    "\n",
    "# Load CSV into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(workspace_file_path)\n",
    "\n",
    "# Check if the table exists\n",
    "if not spark.catalog.tableExists(f\"{catalog}.{db}.eval_dataset\"):\n",
    "    # Write the DataFrame to a managed Unity Catalog table\n",
    "    df.write.format(\"delta\").saveAsTable(f\"{catalog}.{db}.eval_dataset\")\n",
    "else:\n",
    "    print(f\"Table {catalog}.{db}.{eval_table} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9817f1d-c7e4-4cd9-bc95-edf6af716c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md-sandbox\n",
    "\n",
    "# 1/ Ingesting and preparing PDF for LLM and Self Managed Vector Search Embeddings\n",
    "\n",
    "## In this example, we will focus on ingesting pdf documents as source for our retrieval process. \n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-0.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "\n",
    "For this example, we will add Databricks ebook PDFs from [Databricks resources page](https://www.databricks.com/resources) to our knowledge database.\n",
    "\n",
    "**Note: This demo is advanced content, we strongly recommend going over the simple version first to learn the basics.**\n",
    "\n",
    "Here are all the detailed steps:\n",
    "\n",
    "- Use autoloader to load the binary PDFs into our first table. \n",
    "- Use the `unstructured` library  to parse the text content of the PDFs.\n",
    "- Use `llama_index` or `Langchain` to split the texts into chuncks.\n",
    "- Compute embeddings for the chunks.\n",
    "- Save our text chunks + embeddings in a Delta Lake table, ready for Vector Search indexing.\n",
    "\n",
    "\n",
    "Lakehouse AI not only provides state of the art solutions to accelerate your AI and LLM projects, but also to accelerate data ingestion and preparation at scale, including unstructured data like PDFs.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F03-advanced-app%2F01-PDF-Advanced-Data-Preparation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F01-PDF-Advanced-Data-Preparation&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229a1c80-6bfd-4c5c-8d9f-1bd32cd46522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingesting Databricks ebook PDFs and extracting their pages\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "First, let's ingest our PDF news data as a Delta Lake table with path and content in binary format. \n",
    "\n",
    "We'll use [Databricks Autoloader](https://docs.databricks.com/en/ingestion/auto-loader/index.html) to incrementally ingeset new files, making it easy to incrementally consume billions of files from the data lake in various data formats. Autoloader easily ingests our unstructured PDF data in binary format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "700b07af-2bd4-44b9-97da-5d08dfcf6805",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingesting PDF files as binary format using Databricks cloudFiles (Autoloader)"
    }
   },
   "outputs": [],
   "source": [
    "# List our raw PDF docs\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}\"\n",
    "\n",
    "print(f\"Reading data from UC Volume: {volume_folder}\")\n",
    "\n",
    "raw_df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .option(\"pathGlobFilter\", \"*.pdf\")\n",
    "    .load(volume_folder+\"/pdfdata\")\n",
    ")\n",
    "\n",
    "display(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754e9cde-3058-4ac6-b041-a07208e9c580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observe that in our raw_df Spark DataFrame above, we have 1 row (for our 1 document), where the contents of that file are in the content column. You may have seen other spark.read formats, but it is common to use the binaryFile format when working with documents (PDFs, DOCX, etc.) then convert the binary contents later.\n",
    "\n",
    "We can now parse out the binary contents! The below code uses a Spark UDF to extract information. While this code could be written as a traditional Python function, adding the @F.udf() decorator allows it to be parallelized across many workers if we have dozens/hundreds/millions of documents to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4742f13b-3f16-4247-b456-791c24d569a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.udf(\n",
    "    returnType=StructType(\n",
    "        [\n",
    "            StructField(\"number_pages\", IntegerType(), nullable=True),\n",
    "            StructField(\"text\", StringType(), nullable=True),\n",
    "            StructField(\"status\", StringType(), nullable=False),\n",
    "        ]\n",
    "    ),\n",
    "    # useArrow=True, # set globally\n",
    ")\n",
    "def parse_pdf(pdf_raw_bytes):\n",
    "    try:\n",
    "        pdf = io.BytesIO(pdf_raw_bytes)\n",
    "        reader = PdfReader(pdf)\n",
    "        output_text = \"\"\n",
    "        for _, page_content in enumerate(reader.pages):\n",
    "            #PyPDF docs: https://pypdf.readthedocs.io/en/stable/user/extract-text.html\n",
    "            output_text += page_content.extract_text( \n",
    "                extraction_mode=\"layout\", \n",
    "                layout_mode_space_vertically=False,\n",
    "                ) + \"\\n\\n\"\n",
    "\n",
    "        return {\n",
    "            \"number_pages\": len(reader.pages),\n",
    "            \"text\": output_text,\n",
    "            \"status\": \"SUCCESS\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"number_pages\": None, \"text\": None, \"status\": f\"ERROR: {e}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "043ee3b4-d0de-42d9-91bb-6da49ee4b3a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply UDF to the binary \"content\" column\n",
    "parsed_df = (\n",
    "  raw_df.withColumn(\"parsed_output\", parse_pdf(\"content\"))\n",
    "        .drop(\"content\") # For brevity\n",
    "        .drop(\"length\")  # For brevity\n",
    ")\n",
    "\n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898b2dfc-d326-4f9b-bdf4-c6e65ae7a6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-2.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "## Extracting our PDF content as text chunks\n",
    "\n",
    "We need to convert the PDF documents bytes to text, and extract chunks from their content.\n",
    "\n",
    "This part can be tricky as PDFs are hard to work with and can be saved as images, for which we'll need an OCR to extract the text.\n",
    "\n",
    "Using the `Unstructured` library within a Spark UDF makes it easy to extract text. \n",
    "\n",
    "*Note: Your cluster will need a few extra libraries that you would typically install with a cluster init script.*\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "### Splitting our big documentation page in smaller chunks\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"700px\">\n",
    "\n",
    "In this demo, some PDFs are very large, with a lot of text.\n",
    "\n",
    "We'll extract the content and then use llama_index `SentenceSplitter`, and ensure that each chunk isn't bigger than 500 tokens. \n",
    "\n",
    "\n",
    "The chunk size and chunk overlap depend on the use case and the PDF files. \n",
    "\n",
    "Remember that your prompt+answer should stay below your model max window size (4096 for llama2). \n",
    "\n",
    "For more details, review the previous [../01-Data-Preparation](01-Data-Preparation) notebook. \n",
    "\n",
    "<br/>\n",
    "<br style=\"clear: both\">\n",
    "<div style=\"background-color: #def2ff; padding: 15px;  border-radius: 30px; \">\n",
    "  <strong>Information</strong><br/>\n",
    "  Remember that the following steps are specific to your dataset. This is a critical part to building a successful RAG assistant.\n",
    "  <br/> Always take time to review the chunks created and ensure they make sense and contain relevant information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83bce67-31ff-452d-8180-3a1434346b66",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "To extract our PDF,  we'll need to setup libraries in our nodes"
    }
   },
   "outputs": [],
   "source": [
    "# Set chunking params\n",
    "chunk_size_tokens = 2000\n",
    "chunk_overlap_tokens = 200\n",
    "\n",
    "# Instantiate tokenizer\n",
    "## Read more here: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "\n",
    "# Create UDF to recursively split text\n",
    "## For other splitting approaches, see accompanying notebook\n",
    "@F.udf(returnType=ArrayType(StringType())\n",
    "          # useArrow=True, # set globally\n",
    "          )\n",
    "def split_char_recursive(content: str) -> List[str]:\n",
    "    # Adding regex to remove ellipsis\n",
    "    pattern = r'\\.{3,}'\n",
    "    cleaned_content = re.sub(pattern, '', content)\n",
    "    # Use Hugging Face's CharacterTextSplitter\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer, \n",
    "        separator = \" \",\n",
    "        chunk_size=chunk_size_tokens, \n",
    "        chunk_overlap=chunk_overlap_tokens\n",
    "    )\n",
    "    chunks = text_splitter.split_text(cleaned_content)\n",
    "    return [doc for doc in chunks]\n",
    "\n",
    "# Apply Chunking\n",
    "chunked_df = (\n",
    "  parsed_df.select(\n",
    "    \"*\", \n",
    "    F.explode(split_char_recursive(\"parsed_output.text\")).alias(\"chunked_text\")\n",
    "  )\n",
    "  .drop(F.col(\"parsed_output\"))\n",
    "  .withColumn(\"chunk_id\", F.md5(F.col(\"chunked_text\")))\n",
    ")\n",
    "\n",
    "# Printouts to review results\n",
    "num_chunks = chunked_df.count()\n",
    "print(f\"Number of chunks: {num_chunks}\")\n",
    "\n",
    "avg_chunk_words = chunked_df.withColumn(\"word_count\", F.size(F.split(F.col(\"chunked_text\"), \" \"))).select(F.avg(F.col(\"word_count\"))).first()[0]\n",
    "print(f\"Average words per chunk: {avg_chunk_words}\")\n",
    "\n",
    "display(chunked_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10aba2a5-2acf-4338-87a4-4339f90fac7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Write the final DataFrame to a Delta table\n",
    "\n",
    "We will save our final results to a table that we will then use as the basis for a vector search index to perform similarlity search. This step is pretty straightforward: we are persisting the results to a permanent location so we can re-use them later. \n",
    "\n",
    "The code below will overwrite so you can run it more than once (but in production, you would append new chunks and/or merge existing ones). We need to enable [Change Data Feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html) to allow Vector Search to monitor for changes tot his table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a70181-dfa4-4920-a102-6e8619f38456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunked_table_name = \"chunked_news\"\n",
    "full_table_location = f\"{catalog}.{db}.{chunked_table_name}\"\n",
    "\n",
    "print(f\"Saving data to UC table: {full_table_location}\")\n",
    "\n",
    "(\n",
    "  chunked_df.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(full_table_location)\n",
    ")\n",
    "\n",
    "# # We need to enable Change Data Feed on our Delta table to use it for Vector Search. If your table was already created, you can alter it:\n",
    "# spark.sql(f\"ALTER TABLE {full_table_location} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033638a4-ac98-4c2b-9b1b-1ed10ad4f3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What's required for our Vector Search Index\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-vector-search-type.png?raw=true\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "Databricks provide multiple types of vector search indexes:\n",
    "\n",
    "- **Managed embeddings**: you provide a text column and endpoint name and Databricks synchronizes the index with your Delta table \n",
    "- **Self Managed embeddings**: you compute the embeddings and save them as a field of your Delta Table, Databricks will then synchronize the index\n",
    "- **Direct index**: when you want to use and update the index without having a Delta Table.\n",
    "\n",
    "In this demo, we will show you how to setup a **Self-managed Embeddings** index. \n",
    "\n",
    "To do so, we will have to first compute the embeddings of our chunks and save them as a Delta Lake table field as `array&ltfloat&gt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0e2a9f-70da-43ec-a08e-6378e70d62f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Introducing Databricks GTE Embeddings Foundation Model endpoints\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-4.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Foundation Models are provided by Databricks, and can be used out-of-the-box.\n",
    "\n",
    "Databricks supports several endpoint types to compute embeddings or evaluate a model:\n",
    "- DBRX Instruct, a **foundation model endpoint**, or another model served by databricks (ex: llama2-70B, MPT...)\n",
    "- An **external endpoint**, acting as a gateway to an external model (ex: Azure OpenAI)\n",
    "- A **custom**, fined-tuned model hosted on Databricks model service\n",
    "\n",
    "Open the [Model Serving Endpoint page](/ml/endpoints) to explore and try the foundation models.\n",
    "\n",
    "For this demo, we will use the foundation model `GTE` (embeddings) and `DBRX` (chat). <br/><br/>\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-foundation-models.png?raw=true\" width=\"600px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44856067-692f-4727-99bf-8b0c15a51c7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using Databricks Foundation model GTE as embedding endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# gte-large-en Foundation models are available using the /serving-endpoints/databricks-gtegte-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "## NOTE: if you change your embedding model here, make sure you change it in the query step too\n",
    "embeddings = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "pprint(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e1171a-3f44-438b-8b14-036a8a96f87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup via UI\n",
    "You can perform this step from the UI: \n",
    "\n",
    "1. Navigate on the left side to Catalog > {your workshop catalog+schema} > Tables\n",
    "\n",
    "2. Click on the table you created in the last step (e.g. `chunked_product_manuals`)\n",
    "\n",
    "3. Once on the table screen: at the top-right, click Create > Vector Search Index\n",
    "\n",
    "4. Fill in these details:\n",
    "  * Enter index name: `product_manuals_index`\n",
    "  * Primary key: `chunk_id`\n",
    "  * Endpoint: `vs_endpoint_1` <-- replace `1` with a different number if errors occur\n",
    "    * If you get an error creating, the index is full and you should create another one.\n",
    "  * Embedding source: `Compute embeddings`\n",
    "  * Embedding source column: `chunked_text`\n",
    "  * Embedding model: `databricks-gte-large-en`\n",
    "  * Sync mode: `Triggered`\n",
    "\n",
    "For more details, see documentation: [Create index using the UI\n",
    "](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-index-using-the-ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b79ef3-84a9-44b1-95c0-0cdf857ea1da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup Vector Search Index programmatically.\n",
    "`You can skip this step if you created the index in the UI`\n",
    "\n",
    "The below code completes two steps: \n",
    "1. Sets up a Vector Search **Endpoint**. This is the **compute** that hosts your index, and an endpoint can host multiple indices\n",
    "2. Sets up a Vector Search **Index**. This is the **online replica** of your Delta table we will use in our RAG application\n",
    "\n",
    "Full documentation: [Create index using the Python SDK](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-index-using-the-python-sdk)\n",
    "\n",
    "**NOTE**: The cell below should run in 5-10 minutes, and will show as completed when the Endpoint is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b278b559-5ca3-46f2-bced-8459d5bc119a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Our dataset is now ready! Let's create our Self-Managed Vector Search Index.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-3.png?raw=true\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "Our dataset is now ready. We chunked the documentation pages into small sections, computed the embeddings and saved it as a Delta Lake table.\n",
    "\n",
    "Next, we'll configure Databricks Vector Search to ingest data from this table.\n",
    "\n",
    "Vector search index uses a Vector search endpoint to serve the embeddings (you can think about it as your Vector Search API endpoint). <br/>\n",
    "Multiple Indexes can use the same endpoint. Let's start by creating one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1db2d57-6253-46a2-a2eb-f0fcdf1a7f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If a vector search index is ready, use the name of the available index and set it up in config, else creare a new vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228c5df6-336f-4fc6-8c13-83fe7f1d8152",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec98c8f-8fb5-4924-9a5b-d3b09b1f2f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "You can view your endpoint on the [Vector Search Endpoints UI](#/setting/clusters/vector-search). Click on the endpoint name to see all indexes that are served by the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5728181b-44d4-4a86-8d84-73f08752b5df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the Self-managed vector search using our endpoint"
    }
   },
   "outputs": [],
   "source": [
    "# One the endpoint is ready, lets create the Index\n",
    "index_name = \"news_index\"\n",
    "full_index_location = f\"{catalog}.{db}.{index_name}\"\n",
    "\n",
    "# Check first to see if index already exists\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, full_index_location):\n",
    "  print(f\"Creating index {full_index_location} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name =                 VECTOR_SEARCH_ENDPOINT_NAME, # The endpoint where you want to host the index\n",
    "    index_name =                    full_index_location, # Where in UC you want the index to be created\n",
    "    source_table_name =             full_table_location, #The UC location of the offline source table\n",
    "    pipeline_type =                 \"TRIGGERED\", # Set so we can manually refresh the index\n",
    "    primary_key =                   \"chunk_id\", # The primary key of each chunk\n",
    "    embedding_source_column =       \"chunked_text\", # The column containing chunked text\n",
    "    embedding_model_endpoint_name = \"databricks-bge-large-en\" # The embedding model we want to use\n",
    "  )\n",
    "  # Creating the index will take a few moments. Navigate to the Catalog UI to take a look!\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, full_index_location)\n",
    "else:\n",
    "  # If the index already exists, let's force a refresh using the .sync() method\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, full_index_location)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, full_index_location).sync()\n",
    "\n",
    "print(f\"index {full_index_location} on table {full_table_location} is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2287bf-f747-4f17-9d2b-2253456a7094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Searching for similar content\n",
    "\n",
    "That's all we have to do. Databricks will automatically capture and synchronize new entries in your Delta Lake Table.\n",
    "\n",
    "Note that depending on your dataset size and model size, index creation can take a few seconds to start and index your embeddings.\n",
    "\n",
    "Let's give it a try and search for similar content.\n",
    "\n",
    "*Note: `similarity_search` also supports a filters parameter. This is useful to add a security layer to your RAG system: you can filter out some sensitive content based on who is doing the call (for example filter on a specific department based on the user preference).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7146073-a70a-43c4-82ec-dcda339fa282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If you have ingested a different PDF document, change this question to something the document mentions\n",
    "question = \"What happened with unitedheathcare?\"\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, full_index_location).similarity_search(\n",
    "  query_text=question,\n",
    "  columns=[\"chunk_id\", \"chunked_text\"],\n",
    "  num_results=2)\n",
    "\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f0a3a07-ca50-4a27-8e60-28afffcef4e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next step: Deploy our chatbot model with RAG\n",
    "\n",
    "We've seen how Databricks Lakehouse AI makes it easy to ingest and prepare your documents, and deploy a Self Managed Vector Search index on top of it with just a few lines of code and configuration.\n",
    "\n",
    "This simplifies and accelerates your data projects so that you can focus on the next step: creating your realtime chatbot endpoint with well-crafted prompt augmentation.\n",
    "\n",
    "Open the [02-Advanced-Chatbot-Chain]($./02-Advanced-Chatbot-Chain) notebook to create and deploy a chatbot endpoint."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1548972885302109,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-PDF-Advanced-Data-Preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
