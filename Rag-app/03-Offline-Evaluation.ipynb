{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "502df7d1-d406-428d-ab6d-59d2980d540e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Turn the Review App logs into an Evaluation Set\n",
    "\n",
    "The Review application captures your user feedbacks.\n",
    "\n",
    "This feedback is saved under 2 tables within your schema.\n",
    "\n",
    "In this notebook, we will show you how to extract the logs from the Review App into an Evaluation Set.  It is important to review each row and ensure the data quality is high e.g., the question is logical and the response makes sense.\n",
    "\n",
    "1. Requests with a üëç :\n",
    "    - `request`: As entered by the user\n",
    "    - `expected_response`: If the user edited the response, that is used, otherwise, the model's generated response.\n",
    "2. Requests with a üëé :\n",
    "    - `request`: As entered by the user\n",
    "    - `expected_response`: If the user edited the response, that is used, otherwise, null.\n",
    "3. Requests without any feedback\n",
    "    - `request`: As entered by the user\n",
    "\n",
    "Across all types of requests, if the user üëç a chunk from the `retrieved_context`, the `doc_uri` of that chunk is included in `expected_retrieved_context` for the question.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F03-advanced-app%2F03-Offline-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F03-Offline-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d54b95f1-bae5-43e1-a902-744f9f3514b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-agents mlflow mlflow-skinny databricks-sdk==0.23.0 transformers==4.41.1 pypdf==4.1.0 langchain==0.2.1 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "494064ed-1528-43c4-a846-6f50b7ae28ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1.1/ Extracting the logs \n",
    "\n",
    "\n",
    "*Note: for now, this part requires a few SQL queries that we provide in this notebook to properly format the review app into training dataset.*\n",
    "\n",
    "*We'll update this notebook soon with an simpler version - stay tuned!*\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F03-advanced-app%2F03-Offline-Evaluation&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F03-Offline-Evaluation&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4959106-87a6-4e52-9f23-7704d1b912c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46e3604-a3cb-4603-9729-9681c090410d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "MODEL_NAME = \"rag_demo_advanced\"\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\"\n",
    "browser_url = mlflow.utils.databricks_utils.get_browser_hostname()\n",
    "\n",
    "# # Get the name of the Inference Tables where logs are stored\n",
    "active_deployments = agents.list_deployments()\n",
    "active_deployment = next((item for item in active_deployments if item.model_name == MODEL_NAME_FQN), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62153d17-9554-4fb5-bd63-2772820454f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1.2/ Our eval dataset is now ready! \n",
    "\n",
    "The review app makes it easy to build & create your evaluation dataset. \n",
    "\n",
    "*Note: the eval app logs may take some time to be available to you. If the dataset is empty, wait a bit.*\n",
    "\n",
    "To simplify the demo and make sure you don't have to craft your own eval dataset, we saved a ready-to-use eval dataset already pre-generated for you. We'll use this one for the demo instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e78edcc-38d8-46b2-a92a-d26fe287f080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_dataset = spark.table(f\"{catalog}.{db}.eval_dataset\")\n",
    "display(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6779c58d-9d99-48bf-95b7-6a39e32d4fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the correct Python environment for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55059826-c8c7-49c8-8bcd-072c1a667808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Retrieve the model we want to eval\n",
    "model = get_latest_model(MODEL_NAME_FQN)\n",
    "pip_requirements = mlflow.pyfunc.get_model_dependencies(f\"runs:/{model.run_id}/chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ab427dc-bae9-4a0b-857e-d3e497902296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $pip_requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a3ced43-91fb-4fe2-9e8e-3f22209c2f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run our evaluation from the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2532e3c3-8dbe-4167-a90e-ae320ff7571a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"eval_dataset_advanced\"):\n",
    "    # Evaluate the logged model\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        model=f'runs:/{model.run_id}/chain',\n",
    "        model_type=\"databricks-agent\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b04bd38f-9ebe-4ddf-bda6-2916d6f984cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can open MLFlow and review the eval metrics, and also compare it to previous eval runs!\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/mlflow-eval.gif?raw=true\" width=\"1200px\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ee364f-e876-4b95-b7a2-c3c3f0a4cc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### This is looking good, let's tag our model as production ready\n",
    "\n",
    "After reviewing the model correctness and potentially comparing its behavior to your other previous version, we can flag our model as ready to be deployed.\n",
    "\n",
    "*Note: Evaluation can be automated and part of a MLOps step: once you deploy a new Chatbot version with a new prompt, run the evaluation job and benchmark your model behavior vs the previous version.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da23b572-1dd4-40e9-9ac4-a25447fdd0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "client.set_registered_model_alias(name=MODEL_NAME_FQN, alias=\"prod\", version=model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22ea444-aeff-411a-b675-0bb805332df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Mosaic AI Agent Evaluation makes it easy to evaluate your LLM Models, leveraging custom metrics.\n",
    "\n",
    "Evaluating your chatbot is key to measure your future version impact, and your Data Intelligence Platform makes it easy leveraging automated Workflow for your MLOps pipelines.\n",
    "\n",
    "For a production-grade GenAI application, this step should be automated and part as a job, executed everytime the model is changed and benchmarked against previous run to make sure you don't have performance regression."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03-Offline-Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
