{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769af9af-a349-4506-9111-9c3ddc5692aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2/ Advanced chatbot with message history and filter using Langchain and DBRX Instruct\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/llm-rag-self-managed-flow-2.png?raw=true\" style=\"float: right; margin-left: 10px\"  width=\"900px;\">\n",
    "\n",
    "Our Vector Search Index is now ready!\n",
    "\n",
    "Let's now create a more advanced langchain model to perform RAG.\n",
    "\n",
    "We will improve our langchain model with the following:\n",
    "\n",
    "- Build a complete chain supporting a chat history, using Databricks DBRX Instruct input style\n",
    "- Add a filter to only answer Databricks-related questions\n",
    "- Compute the embeddings with Databricks BGE models within our chain to query the self-managed Vector Search Index\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F03-advanced-app%2F02-Advanced-Chatbot-Chain&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F03-advanced-app%2F02-Advanced-Chatbot-Chain&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ff044e-d29d-4f92-9d76-9b566bacf5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-agents mlflow-skinny mlflow mlflow[gateway] langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4 databricks-vectorsearch databricks-sdk==0.23.0 transformers==4.41.1 pypdf==4.1.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa97d05e-cf8c-4e51-bba3-96ccd19441e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c4d9a9-621f-4667-b593-6a882f79d50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rag_chain_config = {\n",
    "    \"databricks_resources\": {\n",
    "        \"llm_endpoint_name\": \"databricks-dbrx-instruct\",\n",
    "        \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    },\n",
    "    \"input_example\": {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is Apache Spark\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Apache spark is a distributed, OSS in-memory computation engine.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Does it support streaming?\"}\n",
    "        ]\n",
    "    },\n",
    "    \"llm_config\": {\n",
    "        \"llm_parameters\": {\"max_tokens\": 1500, \"temperature\": 0.01},\n",
    "        \"llm_prompt_template\": \"You are a trusted assistant that helps answer questions based only on the provided information. If you do not know the answer to a question, you truthfully say you do not know.  Here is some context which might or might not help you answer: {context}.  Answer directly, do not repeat the question, do not start with something like: the answer to the question, do not add AI in front of your answer, do not say: here is the answer, do not mention the context or the question. Based on this context, answer this question: {question}\",\n",
    "        \"llm_prompt_template_variables\": [\"context\", \"question\"],\n",
    "    },\n",
    "    \"retriever_config\": {\n",
    "        \"embedding_model\": \"databricks-gte-large-en\",\n",
    "        \"chunk_template\": \"Passage: {chunk_text}\\n\",\n",
    "        \"data_pipeline_tag\": \"poc\",\n",
    "        \"parameters\": {\"k\": 3, \"query_type\": \"ann\"},\n",
    "        \"schema\": {\"chunk_text\": \"chunked_text\", \"document_uri\": \"path\", \"primary_key\": \"chunk_id\"},\n",
    "        \"vector_search_index\": f\"{catalog}.{db}.news_index\",\n",
    "    },\n",
    "}\n",
    "try:\n",
    "    with open('rag_chain_config.yaml', 'w') as f:\n",
    "        yaml.dump(rag_chain_config, f)\n",
    "except:\n",
    "    print('pass to work on build job')\n",
    "model_config = mlflow.models.ModelConfig(development_config='rag_chain_config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f468c38-ead4-465b-bc3e-04d2263f9d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploring Langchain capabilities\n",
    "\n",
    "Let's start with the basics and send a query to a Databricks Foundation Model using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ce5d96-c889-4d29-8829-3eaf444f7b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When invoking our chain, we'll pass history as a list, specifying whether each message was sent by a user or the assistant. For example:\n",
    "\n",
    "```\n",
    "[\n",
    "  {\"role\": \"user\", \"content\": \"What is Apache Spark?\"}, \n",
    "  {\"role\": \"assistant\", \"content\": \"Apache Spark is an open-source data processing engine that is widely used in big data analytics.\"}, \n",
    "  {\"role\": \"user\", \"content\": \"Does it support streaming?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Let's create chain components to transform this input into the inputs passed to `prompt_with_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd90e60-757e-4406-bfb5-f5d87095a760",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Chat History Extractor Chain"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain.py\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "from operator import itemgetter\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "## Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Return the string contents of the most recent message from the user\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "# Return the chat history, which is is everything before the last question\n",
    "def extract_chat_history(chat_messages_array):\n",
    "    return chat_messages_array[:-1]\n",
    "\n",
    "# Load the chain's configuration\n",
    "model_config = mlflow.models.ModelConfig(development_config=\"rag_chain_config.yaml\")\n",
    "\n",
    "databricks_resources = model_config.get(\"databricks_resources\")\n",
    "retriever_config = model_config.get(\"retriever_config\")\n",
    "llm_config = model_config.get(\"llm_config\")\n",
    "\n",
    "# Connect to the Vector Search Index\n",
    "vs_client = VectorSearchClient(disable_notice=True)\n",
    "vs_index = vs_client.get_index(\n",
    "    endpoint_name=databricks_resources.get(\"vector_search_endpoint_name\"),\n",
    "    index_name=retriever_config.get(\"vector_search_index\"),\n",
    ")\n",
    "vector_search_schema = retriever_config.get(\"schema\")\n",
    "\n",
    "embedding_model = DatabricksEmbeddings(endpoint=retriever_config.get(\"embedding_model\"))\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    embedding=embedding_model, \n",
    "    columns=[\n",
    "        vector_search_schema.get(\"primary_key\"),\n",
    "        vector_search_schema.get(\"chunk_text\"),\n",
    "        vector_search_schema.get(\"document_uri\"),\n",
    "    ],\n",
    ").as_retriever(search_kwargs=retriever_config.get(\"parameters\"))\n",
    "\n",
    "# Enable the RAG Studio Review App to properly display retrieved chunks and evaluation suite to measure the retriever\n",
    "mlflow.models.set_retriever_schema(\n",
    "    primary_key=vector_search_schema.get(\"primary_key\"),\n",
    "    text_column=vector_search_schema.get(\"chunk_text\"),\n",
    "    doc_uri=vector_search_schema.get(\"document_uri\")  # Review App uses `doc_uri` to display chunks from the same document in a single view\n",
    ")\n",
    "\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt\n",
    "def format_context(docs):\n",
    "    chunk_template = retriever_config.get(\"chunk_template\")\n",
    "    chunk_contents = [\n",
    "        chunk_template.format(\n",
    "            chunk_text=d.page_content,\n",
    "            document_uri=d.metadata[vector_search_schema.get(\"document_uri\")],\n",
    "        )\n",
    "        for d in docs\n",
    "    ]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "\n",
    "# Prompt Template for generation\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", llm_config.get(\"llm_prompt_template\")),\n",
    "        # Note: This chain does not compress the history, so very long converastions can overflow the context window.\n",
    "        MessagesPlaceholder(variable_name=\"formatted_chat_history\"),\n",
    "        # User's most current question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Format the converastion history to fit into the prompt template above.\n",
    "def format_chat_history_for_prompt(chat_messages_array):\n",
    "    history = extract_chat_history(chat_messages_array)\n",
    "    formatted_chat_history = []\n",
    "    if len(history) > 0:\n",
    "        for chat_message in history:\n",
    "            if chat_message[\"role\"] == \"user\":\n",
    "                formatted_chat_history.append(HumanMessage(content=chat_message[\"content\"]))\n",
    "            elif chat_message[\"role\"] == \"assistant\":\n",
    "                formatted_chat_history.append(AIMessage(content=chat_message[\"content\"]))\n",
    "    return formatted_chat_history\n",
    "\n",
    "\n",
    "# Prompt Template for query rewriting to allow converastion history to work - this will translate a query such as \"how does it work?\" after a question such as \"what is spark?\" to \"how does spark work?\".\n",
    "query_rewrite_template = \"\"\"Based on the chat history below, we want you to generate a query for an external data source to retrieve relevant documents so that we can better answer the question. The query should be in natural language. The external data source uses similarity search to search for relevant documents in a vector space. So the query should be similar to the relevant documents semantically. Answer with only the query. Do not add explanation.\n",
    "\n",
    "Chat history: {chat_history}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    template=query_rewrite_template,\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "# FM for generation\n",
    "model = ChatDatabricks(\n",
    "    endpoint=databricks_resources.get(\"llm_endpoint_name\"),\n",
    "    extra_params=llm_config.get(\"llm_parameters\"),\n",
    ")\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_chat_history),\n",
    "        \"formatted_chat_history\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(format_chat_history_for_prompt),\n",
    "    }\n",
    "    | RunnablePassthrough()\n",
    "    | {\n",
    "        \"context\": RunnableBranch(  # Only re-write the question if there is a chat history\n",
    "            (\n",
    "                lambda x: len(x[\"chat_history\"]) > 0,\n",
    "                query_rewrite_prompt | model | StrOutputParser(),\n",
    "            ),\n",
    "            itemgetter(\"question\"),\n",
    "        )\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "        \"formatted_chat_history\": itemgetter(\"formatted_chat_history\"),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "## Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd866f45-76e4-4dd5-9929-a4423cf584a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=f\"dbdemos_rag_advanced\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "        model_config='rag_chain_config.yaml',  # Chain configuration \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=model_config.get(\"input_example\"),  # Save the chain's input schema.  MLflow will execute the chain before logging & capture it's output schema.\n",
    "        example_no_conversion=True,  # Required by MLflow to use the input_example as the chain's schema\n",
    "    )\n",
    "\n",
    "# Test the chain locally\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(model_config.get(\"input_example\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63bd463a-1d68-46dd-8e20-10d3581c0c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"rag_demo_advanced\"\n",
    "MODEL_NAME_FQN = f\"{catalog}.{db}.{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5aedb1-8190-4c34-85ff-4bd963e7cb34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=MODEL_NAME_FQN)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=MODEL_NAME_FQN, model_version=uc_registered_model_info.version, scale_to_zero=True)\n",
    "\n",
    "instructions_to_reviewer = f\"\"\"### Instructions for Testing the ournews Chatbot assistant\n",
    "\n",
    "Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.\n",
    "\n",
    "1. **Variety of Questions**:\n",
    "   - Please try a wide range of questions that you anticipate the end users of the application will ask. This helps us ensure the application can handle the expected queries effectively.\n",
    "\n",
    "2. **Feedback on Answers**:\n",
    "   - After asking each question, use the feedback widgets provided to review the answer given by the application.\n",
    "   - If you think the answer is incorrect or could be improved, please use \"Edit Answer\" to correct it. Your corrections will enable our team to refine the application's accuracy.\n",
    "\n",
    "3. **Review of Returned Documents**:\n",
    "   - Carefully review each document that the system returns in response to your question.\n",
    "   - Use the thumbs up/down feature to indicate whether the document was relevant to the question asked. A thumbs up signifies relevance, while a thumbs down indicates the document was not useful.\n",
    "\n",
    "Thank you for your time and effort in testing our assistant. Your contributions are essential to delivering a high-quality product to our end users.\"\"\"\n",
    "\n",
    "\n",
    "# Add the user-facing instructions to the Review App\n",
    "agents.set_review_instructions(MODEL_NAME_FQN, instructions_to_reviewer)\n",
    "wait_for_model_serving_endpoint_to_be_ready(deployment_info.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbf723f-2e03-4523-8df9-2f999b91406e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Grant stakeholders access to the Review App\n",
    "\n",
    "Now, grant your stakeholders permissions to use the Review App. To simplify access, stakeholders do not require to have Databricks accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2b20ba-9ee6-444f-b7ef-3371f0f65f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_list = [\"aradhya.chouhan@databricks.com\"]\n",
    "# Set the permissions.\n",
    "agents.set_permissions(model_name=MODEL_NAME_FQN, users=user_list, permission_level=agents.PermissionLevel.CAN_QUERY)\n",
    "\n",
    "print(f\"Share this URL with your stakeholders: {deployment_info.review_app_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2481b474-c84b-4baf-ab91-c8de4119d4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "We've seen how we can improve our chatbot, adding more advanced capabilities to handle a chat history.\n",
    "\n",
    "As you add capabilities to your model and tune the prompt, it will get harder to evaluate your model performance in a repeatable way.\n",
    "\n",
    "Your new prompt might work well for what you tried to fixed, but could also have impact on other questions.\n",
    "\n",
    "## Next: Introducing offline model evaluation with Mosaic AI Agent Evaluation\n",
    "\n",
    "To solve these issue, we need a repeatable way of testing our model answer as part of our LLMOps deployment!\n",
    "\n",
    "Open the next [03-Offline-Evaluation]($./03-Offline-Evaluation) notebook to discover how to evaluate your model."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Advanced-Chatbot-Chain",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
