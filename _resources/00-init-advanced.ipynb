{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "959cf12e-efa3-4c3f-bcac-bf2c24c75d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# init notebook setting up the backend. \n",
    "\n",
    "Do not edit the notebook, it contains import and helpers for the demo\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1444828305810485&notebook=%2F_resources%2F00-init-advanced&demo_name=llm-rag-chatbot&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fllm-rag-chatbot%2F_resources%2F00-init-advanced&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7760d4bd-3f4e-4f9e-848a-0a069000f542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa5f3f9-16b5-4c14-a55b-eef616dad24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os, re, io\n",
    "from pyspark.sql.types import *\n",
    "from transformers import AutoTokenizer\n",
    "from pypdf import PdfReader\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b9e76d-004b-4ada-ad05-e12a3218fbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def deduplicate_assessments_table(assessment_table):\n",
    "    # De-dup response assessments\n",
    "    assessments_request_deduplicated_df = spark.sql(f\"\"\"select * except(row_number)\n",
    "                                        from ( select *, row_number() over (\n",
    "                                                partition by request_id\n",
    "                                                order by\n",
    "                                                timestamp desc\n",
    "                                            ) as row_number from {assessment_table} where text_assessment is not NULL\n",
    "                                        ) where row_number = 1\"\"\")\n",
    "    # De-dup the retrieval assessments\n",
    "    assessments_retrieval_deduplicated_df = spark.sql(f\"\"\"select * except( retrieval_assessment, source, timestamp, text_assessment, schema_version),\n",
    "        any_value(timestamp) as timestamp,\n",
    "        any_value(source) as source,\n",
    "        collect_list(retrieval_assessment) as retrieval_assessments\n",
    "      from {assessment_table} where retrieval_assessment is not NULL group by request_id, source.id, step_id\"\"\"    )\n",
    "\n",
    "    # Merge together\n",
    "    assessments_request_deduplicated_df = assessments_request_deduplicated_df.drop(\"retrieval_assessment\", \"step_id\")\n",
    "    assessments_retrieval_deduplicated_df = assessments_retrieval_deduplicated_df.withColumnRenamed(\"request_id\", \"request_id2\").withColumnRenamed(\"source\", \"source2\").drop(\"step_id\", \"timestamp\")\n",
    "\n",
    "    merged_deduplicated_assessments_df = assessments_request_deduplicated_df.join(\n",
    "        assessments_retrieval_deduplicated_df,\n",
    "        (assessments_request_deduplicated_df.request_id == assessments_retrieval_deduplicated_df.request_id2) &\n",
    "        (assessments_request_deduplicated_df.source.id == assessments_retrieval_deduplicated_df.source2.id),\n",
    "        \"full\"\n",
    "    ).select(\n",
    "        [str(col) for col in assessments_request_deduplicated_df.columns] +\n",
    "        [assessments_retrieval_deduplicated_df.retrieval_assessments]\n",
    "    )\n",
    "\n",
    "    return merged_deduplicated_assessments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d954de4d-0b73-4d7c-8f70-8505b7128766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_latest_model(model_name):\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    latest_version = None\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if not latest_version or version_int > int(latest_version.version):\n",
    "            latest_version = mv\n",
    "    return latest_version"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-init-advanced",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
